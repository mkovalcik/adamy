{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Adamy Valuation - Michigan Data Science Team\n",
    "### Michael Xinyu Tim Sid Derek Manny\n",
    "                                                                                                        5/06/2017"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Purpose:** This notebook describes the data processing pipeline we have built and instructions on how to interpret the results.  \n",
    "  \n",
    "**Conclusion:** Ebitda/interest_exp is the best predictor of ev/ebidta. It seems that the higher the ebitda/interest_exp the lower our prediction for ev/ebitda. Other important features are ebitda_margin and ebitda.  \n",
    "  \n",
    "**Notes:** It was hard to precisely quantify the degree to which these variables actually impacted ev/ebitda because of the limitations of linear regression. We have provided several lists of features that our models found to be important. It would be interesting to compare and contrast these lists with the existing literature. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from get_data import *\n",
    "\n",
    "%matplotlib inline\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pylab as plt\n",
    "import seaborn as sns\n",
    "import scipy.stats as stats\n",
    "import random\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from get_data import *\n",
    "\n",
    "# SciKit Learn Modules\n",
    "from sklearn.preprocessing import scale\n",
    "from sklearn.linear_model import LinearRegression, Ridge, RidgeCV, Lasso, LassoCV\n",
    "from sklearn.model_selection import cross_val_score, train_test_split\n",
    "from sklearn import metrics\n",
    "from sklearn.ensemble import RandomForestRegressor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "np.random.seed(0)\n",
    "raw_data_frame = load_data_frames()\n",
    "sectors = ['consumerDiscrete', 'consumerStaples', 'industrials']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our exploratory data anlaysis led us to discover that our models would be more effective after transforming some of the columns. Specifically those columns that represent monetary values as well as a few others."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0, 't_ev')\n",
      "(1, 't_rev')\n",
      "(2, 'ebitda')\n",
      "(3, 'ebitda_margin')\n",
      "(4, 't_ev/t_rev')\n",
      "(5, 't_ev/ebitda')\n",
      "(6, 'est_ann_rev_gr_1yr')\n",
      "(7, 'est_ann_ebitda_gr_1yr')\n",
      "(8, 't_rev_1yr_growth')\n",
      "(9, 'ebitda_1yr_growth')\n",
      "(10, 't_rev_3_yr_cagr')\n",
      "(11, 'ebitda_3yr_cagr')\n",
      "(12, 't_rev_5yr_cagr')\n",
      "(13, 'ebitda_5yr_cagr')\n",
      "(14, 'return_on_assets')\n",
      "(15, 'return_on_equity')\n",
      "(16, 'capex_as_percent_rev')\n",
      "(17, 'ebitda/interest_exp')\n",
      "(18, 't_debt/cap_percent')\n",
      "(19, 't_debt/equity_percent')\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(raw_data_frame[0].columns)):\n",
    "    print(i, raw_data_frame[0].columns[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['t_ev', 't_rev', 'ebitda', 't_ev/ebitda', 'capex_as_percent_rev', 'ebitda/interest_exp', 't_debt/cap_percent', 't_debt/equity_percent']\n"
     ]
    }
   ],
   "source": [
    "log_transform_columns =  [0, 1, 2, 5, 16, 17, 18, 19]\n",
    "log_transform_col_names = []\n",
    "for col in log_transform_columns:\n",
    "    log_transform_col_names.append(raw_data_frame[0].columns[col])\n",
    "print log_transform_col_names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to actually modify the data for the columns we discovered.  \n",
    "**Note:** Because we do the following process in place it is important that the line is not run twice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# log-transform inplace\n",
    "for i in range(len(sectors)):\n",
    "    raw_data_frame[i][log_transform_col_names] = raw_data_frame[i][log_transform_col_names].apply(np.log10)\n",
    "    raw_data_frame[i].rename(columns = {\n",
    "        't_ev':'log_t_ev',\n",
    "        't_rev':'log_t_rev',\n",
    "        'ebitda':'log_ebitda',\n",
    "        't_ev/ebitda':'log_t_ev/ebitda',\n",
    "        'capex_as_percent_rev':'log_capex_as_percent_rev',\n",
    "        'ebitda/interest_exp':'log_ebitda/interest_exp',\n",
    "        't_debt/cap_percent':'log_t_debt/cap_percent',\n",
    "        't_debt/equity_percent':'log_t_debt/equity_percent'\n",
    "                                },inplace = True)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to handle missing data we fill in cells the with mean for the column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for i in range(len(sectors)):\n",
    "    raw_data_frame[i].fillna(raw_data_frame[i].mean(), inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Prepare the data for ML models\n",
    "\n",
    "- Target: 't_ev/ebitda'\n",
    "- Discard 't_ev'\n",
    "\n",
    "From pearson correlation heatmap, we see that target variable is not strongly linearly associated with other features. Therefore, the nonlinear methods, like tree-based methods, or SVM with nonlinear kernels will outperform. However, one thing to notice is that our final goal is not to predict the target value but rather to understand the complex relationship between ev/ebitda and other features. We shall try various methods  \n",
    "  \n",
    "We chose to discared the feature t_ev and t_ev/t_rev because we believe it will not be known for a private company"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# drop 't_ev' columns\n",
    "for i in range(len(sectors)):\n",
    "    raw_data_frame[i].drop('log_t_ev', axis=1, inplace=True)\n",
    "    raw_data_frame[i].drop('t_ev/t_rev', axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Let's look at consumerDiscrete\n"
     ]
    }
   ],
   "source": [
    "print(\"Let's look at \" + sectors[0])\n",
    "cur_df = raw_data_frame[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>log_t_rev</th>\n",
       "      <th>log_ebitda</th>\n",
       "      <th>ebitda_margin</th>\n",
       "      <th>est_ann_rev_gr_1yr</th>\n",
       "      <th>est_ann_ebitda_gr_1yr</th>\n",
       "      <th>t_rev_1yr_growth</th>\n",
       "      <th>ebitda_1yr_growth</th>\n",
       "      <th>t_rev_3_yr_cagr</th>\n",
       "      <th>ebitda_3yr_cagr</th>\n",
       "      <th>t_rev_5yr_cagr</th>\n",
       "      <th>ebitda_5yr_cagr</th>\n",
       "      <th>return_on_assets</th>\n",
       "      <th>return_on_equity</th>\n",
       "      <th>log_capex_as_percent_rev</th>\n",
       "      <th>log_ebitda/interest_exp</th>\n",
       "      <th>log_t_debt/cap_percent</th>\n",
       "      <th>log_t_debt/equity_percent</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3.075182</td>\n",
       "      <td>1.897077</td>\n",
       "      <td>6.64</td>\n",
       "      <td>4.540000</td>\n",
       "      <td>16.700000</td>\n",
       "      <td>2.06000</td>\n",
       "      <td>-0.852000</td>\n",
       "      <td>16.400000</td>\n",
       "      <td>19.400000</td>\n",
       "      <td>11.200000</td>\n",
       "      <td>18.700000</td>\n",
       "      <td>4.600000</td>\n",
       "      <td>9.900000</td>\n",
       "      <td>0.460898</td>\n",
       "      <td>1.103804</td>\n",
       "      <td>1.513218</td>\n",
       "      <td>1.685742</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2.426511</td>\n",
       "      <td>0.772322</td>\n",
       "      <td>2.22</td>\n",
       "      <td>7.132813</td>\n",
       "      <td>17.742803</td>\n",
       "      <td>-1.93000</td>\n",
       "      <td>13.234825</td>\n",
       "      <td>-4.040000</td>\n",
       "      <td>-34.500000</td>\n",
       "      <td>-10.600000</td>\n",
       "      <td>-28.900000</td>\n",
       "      <td>-1.570000</td>\n",
       "      <td>-12.200000</td>\n",
       "      <td>0.444045</td>\n",
       "      <td>1.004171</td>\n",
       "      <td>1.561461</td>\n",
       "      <td>1.781483</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3.509740</td>\n",
       "      <td>2.665393</td>\n",
       "      <td>14.30</td>\n",
       "      <td>1.480000</td>\n",
       "      <td>4.270000</td>\n",
       "      <td>4.08000</td>\n",
       "      <td>4.980000</td>\n",
       "      <td>13.000000</td>\n",
       "      <td>9.600000</td>\n",
       "      <td>10.300000</td>\n",
       "      <td>11.200000</td>\n",
       "      <td>9.930000</td>\n",
       "      <td>9.950000</td>\n",
       "      <td>0.278754</td>\n",
       "      <td>1.281033</td>\n",
       "      <td>1.489958</td>\n",
       "      <td>1.650308</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3.531900</td>\n",
       "      <td>2.415808</td>\n",
       "      <td>7.65</td>\n",
       "      <td>-5.230000</td>\n",
       "      <td>-36.900000</td>\n",
       "      <td>-3.46000</td>\n",
       "      <td>-19.200000</td>\n",
       "      <td>-7.400000</td>\n",
       "      <td>-21.700000</td>\n",
       "      <td>-3.080000</td>\n",
       "      <td>-15.300000</td>\n",
       "      <td>1.560000</td>\n",
       "      <td>1.330000</td>\n",
       "      <td>0.609594</td>\n",
       "      <td>1.060698</td>\n",
       "      <td>1.311754</td>\n",
       "      <td>1.411620</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3.191898</td>\n",
       "      <td>2.244277</td>\n",
       "      <td>11.30</td>\n",
       "      <td>7.132813</td>\n",
       "      <td>17.742803</td>\n",
       "      <td>331.70347</td>\n",
       "      <td>13.234825</td>\n",
       "      <td>12.670764</td>\n",
       "      <td>8.528391</td>\n",
       "      <td>8.063843</td>\n",
       "      <td>7.665472</td>\n",
       "      <td>6.344578</td>\n",
       "      <td>14.769923</td>\n",
       "      <td>0.187521</td>\n",
       "      <td>0.481443</td>\n",
       "      <td>1.857332</td>\n",
       "      <td>2.411114</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   log_t_rev  log_ebitda  ebitda_margin  est_ann_rev_gr_1yr  \\\n",
       "0   3.075182    1.897077           6.64            4.540000   \n",
       "1   2.426511    0.772322           2.22            7.132813   \n",
       "2   3.509740    2.665393          14.30            1.480000   \n",
       "3   3.531900    2.415808           7.65           -5.230000   \n",
       "4   3.191898    2.244277          11.30            7.132813   \n",
       "\n",
       "   est_ann_ebitda_gr_1yr  t_rev_1yr_growth  ebitda_1yr_growth  \\\n",
       "0              16.700000           2.06000          -0.852000   \n",
       "1              17.742803          -1.93000          13.234825   \n",
       "2               4.270000           4.08000           4.980000   \n",
       "3             -36.900000          -3.46000         -19.200000   \n",
       "4              17.742803         331.70347          13.234825   \n",
       "\n",
       "   t_rev_3_yr_cagr  ebitda_3yr_cagr  t_rev_5yr_cagr  ebitda_5yr_cagr  \\\n",
       "0        16.400000        19.400000       11.200000        18.700000   \n",
       "1        -4.040000       -34.500000      -10.600000       -28.900000   \n",
       "2        13.000000         9.600000       10.300000        11.200000   \n",
       "3        -7.400000       -21.700000       -3.080000       -15.300000   \n",
       "4        12.670764         8.528391        8.063843         7.665472   \n",
       "\n",
       "   return_on_assets  return_on_equity  log_capex_as_percent_rev  \\\n",
       "0          4.600000          9.900000                  0.460898   \n",
       "1         -1.570000        -12.200000                  0.444045   \n",
       "2          9.930000          9.950000                  0.278754   \n",
       "3          1.560000          1.330000                  0.609594   \n",
       "4          6.344578         14.769923                  0.187521   \n",
       "\n",
       "   log_ebitda/interest_exp  log_t_debt/cap_percent  log_t_debt/equity_percent  \n",
       "0                 1.103804                1.513218                   1.685742  \n",
       "1                 1.004171                1.561461                   1.781483  \n",
       "2                 1.281033                1.489958                   1.650308  \n",
       "3                 1.060698                1.311754                   1.411620  \n",
       "4                 0.481443                1.857332                   2.411114  "
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = cur_df['log_t_ev/ebitda']\n",
    "X = cur_df.drop('log_t_ev/ebitda', axis=1)\n",
    "X.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is common practice in Machine Learning to use inputs of mean = 0 and standard deviation = 1. This way the impact of the features can be compared head to head. The scale() function below does this transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import scale\n",
    "X_scale = scale(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we start doing any model selection it is important that we break our data up into a training and a test set. The training set will be used to select and train the models. The test set will come in at the end of the program and will be our way of testing a models accuracy on data that is has never seen.  \n",
    "The training set will consist of 75% of our data and the test set will have the remaining 25%. Each set has an \"X\" and \"y\" component. The y component is what we are trying to predict, t_ev/ebitda. The X component consists of all other variables in the data set (ebitda, ebitda_margin, est_ann_rev_gr_1yr, etc.) which we will be using to predict t_ev/ebitda."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "x_train, x_test, y_train, y_test = train_test_split(X_scale,y,train_size = 0.75)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn import metrics\n",
    "\n",
    "# ols_model = LinearRegression(normalize=True)\n",
    "# scores = cross_val_score(ols_model, X_scale, y, cv=5, scoring=\"neg_mean_squared_error\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We start by trying two variations of Linear Regression. These variations are very common and are known as Lasso Regression and Ridge Regression. The basic idea is that by imposing certain penalties onto normal linear regression we can find a better model. However, inorder to find the optimal impact of the penatly term (alpha) we must test a series of options and choose the one that performs best. This procedure for doing this is called cross validation, a very common machine learning technique for selecting penalty terms and other \"hyper parameters\". "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average MSE (alpha = 0.010): 0.0679 (+/- 0.0102)\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAZUAAAEeCAYAAABCLIggAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\ni7IAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x11521b9d0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "lasso_model = Lasso()\n",
    "alphas         = [ 1e-5, 1e-4, 1e-3, 5e-3, 1e-2, 5e-2, 1e-1, 1, 10, 100 ]\n",
    "lasso_mses     = []\n",
    "lasso_avg_mses = []\n",
    "lasso_std_mses = []\n",
    "\n",
    "for alpha in alphas:\n",
    "    lasso_model.alpha = alpha\n",
    "    scores = cross_val_score(lasso_model, x_train, y_train, scoring='neg_mean_squared_error', cv=5)\n",
    "    lasso_mses.append( scores*-1 )\n",
    "    lasso_avg_mses.append( -scores.mean() )\n",
    "    lasso_std_mses.append( scores.std() )\n",
    "\n",
    "# Plot Hyperparameter Selection\n",
    "plt.semilogx( alphas, lasso_avg_mses )\n",
    "plt.title('Lasso - MSE vs Alpha')\n",
    "plt.xlabel('Alpha')\n",
    "plt.ylabel('MSE')\n",
    "\n",
    "min_mse = min(lasso_avg_mses)\n",
    "min_mse_index = lasso_avg_mses.index(min(lasso_avg_mses))\n",
    "min_std_index = lasso_std_mses.index(min(lasso_std_mses))\n",
    "\n",
    "if min_mse_index == min_std_index:\n",
    "    best_index = min_mse_index\n",
    "else:\n",
    "    if abs(lasso_avg_mses[min_mse_index] - lasso_avg_mses[min_std_index]) < 0.01*lasso_avg_mses[min_mse_index]:\n",
    "        best_index = min_std_index    \n",
    "    else:\n",
    "        best_index = min_mse_index\n",
    "            \n",
    "    \n",
    "lasso_alpha = alphas[best_index]\n",
    "print(\"Average MSE (alpha = %0.3f): %0.4f (+/- %0.4f)\" % (alphas[best_index],lasso_avg_mses[best_index], lasso_std_mses[best_index]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The whole purpose of the above is to find the optimal alpha (the penalty). For each different alpha the model has a different performance. In the plot above we are measuring the mean squared error of the model for each alpha we tested. The lower the error the better the alpha. Alpha in this case was selected to be 0.010  \n",
    "  \n",
    "Now we are going to perform the same cross validation procedue to find the optimal alpha for Ridge Regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average MSE (alpha = 100.000): 0.0654 (+/- 0.0142)\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYkAAAEeCAYAAAB/vulGAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1179963d0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "ridge_model = Ridge()\n",
    "alphas         = [ 1e-5, 1e-4, 1e-3, 1e-2, 1e-1, 1, 10, 100,1000 ]\n",
    "ridge_mses     = []\n",
    "ridge_avg_mses = []\n",
    "ridge_std_mses = []\n",
    "\n",
    "for alpha in alphas:\n",
    "    ridge_model.alpha = alpha\n",
    "    scores = cross_val_score(ridge_model, X_scale, y, scoring='neg_mean_squared_error', cv=5)\n",
    "    ridge_mses.append( scores*-1 )\n",
    "    ridge_avg_mses.append( -scores.mean() )\n",
    "    ridge_std_mses.append( scores.std() )\n",
    "\n",
    "# Plot Hyperparameter Selection\n",
    "plt.semilogx( alphas, ridge_avg_mses )\n",
    "plt.title('Ridge - MSE vs Alpha')\n",
    "plt.xlabel('Alpha')\n",
    "plt.ylabel('MSE')\n",
    "\n",
    "\n",
    "\n",
    "min_mse = min(ridge_avg_mses)\n",
    "min_mse_index = ridge_avg_mses.index(min(ridge_avg_mses))\n",
    "min_std_index = ridge_std_mses.index(min(ridge_std_mses))\n",
    "\n",
    "if min_mse_index == min_std_index:\n",
    "    best_index = min_mse_index\n",
    "else:\n",
    "    if abs(lasso_avg_mses[min_mse_index] - lasso_avg_mses[min_std_index]) < 0.01*lasso_avg_mses[min_mse_index]:\n",
    "        best_index = min_std_index    \n",
    "    else:\n",
    "        best_index = min_mse_index\n",
    "\n",
    "ridge_alpha = alphas[best_index]        \n",
    "# print('MSEs (alpha = 1e-2):', lasso_mses[3]*-1)\n",
    "print(\"Average MSE (alpha = %0.3f): %0.4f (+/- %0.4f)\" % (alphas[best_index],ridge_avg_mses[best_index], ridge_std_mses[best_index]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For Ridge Regression we found the model performs best with an alpha = 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training the Models\n",
    "Now that we have determined what alpha level is best for both Lasso and Ridge regression we are ready to fit a model to our training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lasso_model = Lasso(alpha = lasso_alpha)\n",
    "lasso_model.fit(x_train,y_train)\n",
    "lasso_betas = lasso_model.coef_\n",
    "lasso_pred = lasso_model.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ridge_model = Ridge(alpha = ridge_alpha)\n",
    "ridge_model.fit(x_train,y_train)\n",
    "ridge_betas = ridge_model.coef_\n",
    "ridge_pred = ridge_model.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lasso Mean Square Err: 0.0531800178845\n",
      "Ridge Mean Square Err: 0.0579797624261\n"
     ]
    }
   ],
   "source": [
    "lasso_mse = metrics.mean_squared_error(y_true = y_test, y_pred = lasso_pred)\n",
    "ridge_mse = metrics.mean_squared_error(y_true = y_test, y_pred = ridge_pred)\n",
    "print \"Lasso Mean Square Err:\",lasso_mse\n",
    "print \"Ridge Mean Square Err:\",ridge_mse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Both model seem to do fairly well. We aren't too concerned with the predictions because the Comps approach to market valuation is much more sophisticated and accurate. However, we can take a look at which features our models found to be most important by looking at the \"beta\" variables we stored above. Beta's are the coefficents in a linear regression model. There is one Beta for every feature (ebitda, ebitda_margin, est_ann_rev_gr_1yr, etc.) that we used in our model. Looking at the absolute value and ranking them will tell us which features had the biggest impact on prediction. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 log_ebitda/interest_exp -0.053\n",
      "2 est_ann_rev_gr_1yr 0.0355\n",
      "3 t_rev_5yr_cagr 0.0337\n",
      "4 log_ebitda -0.0293\n",
      "5 ebitda_margin -0.0195\n"
     ]
    }
   ],
   "source": [
    "lasso_beta_pairs = []\n",
    "for i in range(len(lasso_betas)):\n",
    "    lasso_beta_pairs.append((i,lasso_betas[i])) \n",
    "sorted_lasso_betas = sorted(lasso_beta_pairs, key=lambda x: abs(x[1]), reverse = True) \n",
    "for i in range(5):\n",
    "    index = sorted_lasso_betas[i][0]\n",
    "    print i+1, X.columns[index], round(sorted_lasso_betas[i][1],4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Listed above are the top 5 most important features for predicting the log(t_ev/ebitda) according to lasso model. Because of all that preprocessing we did it is kind of hard to interpret what these numbers actually mean. The **important thing to notice** here is the **order in which the features are ranked and whether a feature is positive is negative.** According to lasso model log(ebitda/interest_exp) is the best predictor of t_ev/ebitda. The coefficent is also negative which means the larger our value of log(ebitda/interest_exp) the smaller our prediction for log(t_ev/ebitda) (I will explain the precise interpretation later). We also see that est_ann_rev_gr_1yr is the second best and t_rev_5yr_cargr is third and so on."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We repeat this process for the ridge model..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 log_ebitda/interest_exp -0.0477\n",
      "2 est_ann_rev_gr_1yr 0.0358\n",
      "3 t_rev_5yr_cagr 0.0347\n",
      "4 log_ebitda -0.0317\n",
      "5 ebitda_1yr_growth -0.0233\n"
     ]
    }
   ],
   "source": [
    "ridge_beta_pairs = []\n",
    "for i in range(len(ridge_betas)):\n",
    "    ridge_beta_pairs.append((i,ridge_betas[i])) \n",
    "sorted_ridge_betas = sorted(ridge_beta_pairs, key=lambda x: abs(x[1]), reverse = True) \n",
    "for i in range(5):\n",
    "    index = sorted_ridge_betas[i][0]\n",
    "    print i+1, X.columns[index], round(sorted_ridge_betas[i][1],4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "According to our ridge regression model log(ebitda/interest_exp) is also our best predictor. Both lasso and ridge have similar features in their top 5 also the order was a bit different."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Intrepretation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Standardizing our inputs gave us the ability to determine which model performed the best and which features were the most important but it made things hard to interpret. Now that we know what features to look at we are going to run a standard linear regression and interpret the coefficients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>log_t_rev</th>\n",
       "      <th>log_ebitda</th>\n",
       "      <th>ebitda_margin</th>\n",
       "      <th>est_ann_rev_gr_1yr</th>\n",
       "      <th>est_ann_ebitda_gr_1yr</th>\n",
       "      <th>t_rev_1yr_growth</th>\n",
       "      <th>ebitda_1yr_growth</th>\n",
       "      <th>t_rev_3_yr_cagr</th>\n",
       "      <th>ebitda_3yr_cagr</th>\n",
       "      <th>t_rev_5yr_cagr</th>\n",
       "      <th>ebitda_5yr_cagr</th>\n",
       "      <th>return_on_assets</th>\n",
       "      <th>return_on_equity</th>\n",
       "      <th>log_capex_as_percent_rev</th>\n",
       "      <th>log_ebitda/interest_exp</th>\n",
       "      <th>log_t_debt/cap_percent</th>\n",
       "      <th>log_t_debt/equity_percent</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3.075182</td>\n",
       "      <td>1.897077</td>\n",
       "      <td>6.64</td>\n",
       "      <td>4.540000</td>\n",
       "      <td>16.700000</td>\n",
       "      <td>2.06000</td>\n",
       "      <td>-0.852000</td>\n",
       "      <td>16.400000</td>\n",
       "      <td>19.400000</td>\n",
       "      <td>11.200000</td>\n",
       "      <td>18.700000</td>\n",
       "      <td>4.600000</td>\n",
       "      <td>9.900000</td>\n",
       "      <td>0.460898</td>\n",
       "      <td>1.103804</td>\n",
       "      <td>1.513218</td>\n",
       "      <td>1.685742</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2.426511</td>\n",
       "      <td>0.772322</td>\n",
       "      <td>2.22</td>\n",
       "      <td>7.132813</td>\n",
       "      <td>17.742803</td>\n",
       "      <td>-1.93000</td>\n",
       "      <td>13.234825</td>\n",
       "      <td>-4.040000</td>\n",
       "      <td>-34.500000</td>\n",
       "      <td>-10.600000</td>\n",
       "      <td>-28.900000</td>\n",
       "      <td>-1.570000</td>\n",
       "      <td>-12.200000</td>\n",
       "      <td>0.444045</td>\n",
       "      <td>1.004171</td>\n",
       "      <td>1.561461</td>\n",
       "      <td>1.781483</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3.509740</td>\n",
       "      <td>2.665393</td>\n",
       "      <td>14.30</td>\n",
       "      <td>1.480000</td>\n",
       "      <td>4.270000</td>\n",
       "      <td>4.08000</td>\n",
       "      <td>4.980000</td>\n",
       "      <td>13.000000</td>\n",
       "      <td>9.600000</td>\n",
       "      <td>10.300000</td>\n",
       "      <td>11.200000</td>\n",
       "      <td>9.930000</td>\n",
       "      <td>9.950000</td>\n",
       "      <td>0.278754</td>\n",
       "      <td>1.281033</td>\n",
       "      <td>1.489958</td>\n",
       "      <td>1.650308</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3.531900</td>\n",
       "      <td>2.415808</td>\n",
       "      <td>7.65</td>\n",
       "      <td>-5.230000</td>\n",
       "      <td>-36.900000</td>\n",
       "      <td>-3.46000</td>\n",
       "      <td>-19.200000</td>\n",
       "      <td>-7.400000</td>\n",
       "      <td>-21.700000</td>\n",
       "      <td>-3.080000</td>\n",
       "      <td>-15.300000</td>\n",
       "      <td>1.560000</td>\n",
       "      <td>1.330000</td>\n",
       "      <td>0.609594</td>\n",
       "      <td>1.060698</td>\n",
       "      <td>1.311754</td>\n",
       "      <td>1.411620</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3.191898</td>\n",
       "      <td>2.244277</td>\n",
       "      <td>11.30</td>\n",
       "      <td>7.132813</td>\n",
       "      <td>17.742803</td>\n",
       "      <td>331.70347</td>\n",
       "      <td>13.234825</td>\n",
       "      <td>12.670764</td>\n",
       "      <td>8.528391</td>\n",
       "      <td>8.063843</td>\n",
       "      <td>7.665472</td>\n",
       "      <td>6.344578</td>\n",
       "      <td>14.769923</td>\n",
       "      <td>0.187521</td>\n",
       "      <td>0.481443</td>\n",
       "      <td>1.857332</td>\n",
       "      <td>2.411114</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   log_t_rev  log_ebitda  ebitda_margin  est_ann_rev_gr_1yr  \\\n",
       "0   3.075182    1.897077           6.64            4.540000   \n",
       "1   2.426511    0.772322           2.22            7.132813   \n",
       "2   3.509740    2.665393          14.30            1.480000   \n",
       "3   3.531900    2.415808           7.65           -5.230000   \n",
       "4   3.191898    2.244277          11.30            7.132813   \n",
       "\n",
       "   est_ann_ebitda_gr_1yr  t_rev_1yr_growth  ebitda_1yr_growth  \\\n",
       "0              16.700000           2.06000          -0.852000   \n",
       "1              17.742803          -1.93000          13.234825   \n",
       "2               4.270000           4.08000           4.980000   \n",
       "3             -36.900000          -3.46000         -19.200000   \n",
       "4              17.742803         331.70347          13.234825   \n",
       "\n",
       "   t_rev_3_yr_cagr  ebitda_3yr_cagr  t_rev_5yr_cagr  ebitda_5yr_cagr  \\\n",
       "0        16.400000        19.400000       11.200000        18.700000   \n",
       "1        -4.040000       -34.500000      -10.600000       -28.900000   \n",
       "2        13.000000         9.600000       10.300000        11.200000   \n",
       "3        -7.400000       -21.700000       -3.080000       -15.300000   \n",
       "4        12.670764         8.528391        8.063843         7.665472   \n",
       "\n",
       "   return_on_assets  return_on_equity  log_capex_as_percent_rev  \\\n",
       "0          4.600000          9.900000                  0.460898   \n",
       "1         -1.570000        -12.200000                  0.444045   \n",
       "2          9.930000          9.950000                  0.278754   \n",
       "3          1.560000          1.330000                  0.609594   \n",
       "4          6.344578         14.769923                  0.187521   \n",
       "\n",
       "   log_ebitda/interest_exp  log_t_debt/cap_percent  log_t_debt/equity_percent  \n",
       "0                 1.103804                1.513218                   1.685742  \n",
       "1                 1.004171                1.561461                   1.781483  \n",
       "2                 1.281033                1.489958                   1.650308  \n",
       "3                 1.060698                1.311754                   1.411620  \n",
       "4                 0.481443                1.857332                   2.411114  "
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = cur_df['log_t_ev/ebitda']\n",
    "X = cur_df.drop('log_t_ev/ebitda', axis=1)\n",
    "X.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x_train, x_test, y_train, y_test = train_test_split(X,y,train_size = 0.75)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ols_model = LinearRegression()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ols_model.fit(X,y)\n",
    "betas = ols_model.coef_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "beta_pairs = []\n",
    "for i in range(len(betas)):\n",
    "    beta_pairs.append((i,ridge_betas[i])) \n",
    "    index = betas[i]\n",
    "    # print i+1, X.columns[i], round(betas[i],7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "log_ebitda/interest_exp -0.1119\n",
      "est_ann_rev_gr_1yr 0.0019\n",
      "t_rev_5yr_cagr 0.0046\n",
      "log_ebitda 0.0094\n",
      "ebitda_margin -0.0016\n",
      "ebitda_1yr_growth -0.0001\n"
     ]
    }
   ],
   "source": [
    "print 'log_ebitda/interest_exp', round(betas[14],4)\n",
    "print 'est_ann_rev_gr_1yr', round(betas[3],4)\n",
    "print 't_rev_5yr_cagr' , round(betas[9],4)\n",
    "print 'log_ebitda', round(betas[1],4)\n",
    "print 'ebitda_margin', round(betas[2],4)\n",
    "print 'ebitda_1yr_growth', round(betas[6],4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking at the six different variables that turned up from our models. The interpretation of the coefficient is as follows:    \n",
    "  \n",
    "**log_ebitda/interest_exp:**  \n",
    "Holding the values for all other features constant, every unit increase we measure in the log_ebitda/interest_exp our prediction of the log(t_ev/ebitda) of to **decrease** by 0.1119 or the t_ev/ebitda to decrease by 10^0.1119 = 1.293 \n",
    "  \n",
    "**est_ann_rev_gr_1yr:**  \n",
    "Holding the values of all other features constant, every unit increase we measure in the est_ann_rev_gr_1yr our prediction of the log(t_ev/ebitda) increases by 0.0019 or in other words our prediction of t_ev/ebitda increases by 10^0.0019 = 1.004  \n",
    "  \n",
    "The same interperation follows for all of these coefficients. I am not sure how useful the actual interpretations are because in the valuation setting there is no way to hold the other features constant. Atleast 3 or 4 features use ebitda and by changing one of those variables you inherently must change some of the others. There is one more method that may be able to exploit this inherent connection between features. It is known as random forest. It is a 'tree based method' I am not an expert on how it works (I will link a suitable explanation) but I know that it produces a list of what features were most important in its prediction. It cannot tell you whether a certain feature has a positive or negative impact on the prediction."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forest Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average MSE: 0.0520 (+/- 0.0169)\n"
     ]
    }
   ],
   "source": [
    "# basic model \n",
    "model = RandomForestRegressor()\n",
    "scores = cross_val_score(model, x_train, y_train, scoring='neg_mean_squared_error', cv=5)\n",
    "# print('MSEs:', scores*-1)\n",
    "print(\"Average MSE: %0.4f (+/- %0.4f)\" % (-scores.mean(), scores.std()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are a bunch of 'tuning parameters' for random forest. One commonly used method is to just try a bunch of combinations and take the the one that works best."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from itertools import product\n",
    "n_estimators_params = [10, 20]\n",
    "max_features_params = ['auto', 'sqrt', \"log2\"]\n",
    "min_samples_split_params = [2]\n",
    "min_samples_leaf_params = [  2]\n",
    "\n",
    "result_dict = {}\n",
    "for param in product(n_estimators_params, max_features_params, min_samples_split_params, min_samples_leaf_params):\n",
    "    rf_model = RandomForestRegressor(n_estimators=param[0], max_features=param[1],\n",
    "                                min_samples_split=param[2], min_samples_leaf=param[3], \n",
    "                                 random_state=1711, n_jobs=-1)\n",
    "    scores = cross_val_score(rf_model, x_train, y_train, scoring='neg_mean_squared_error', cv=5)\n",
    "    result_dict[param] = (-scores.mean(), scores.std())\n",
    "#     print 'current param: n_estimators_params, max_features_params, min_samples_split_params, min_samples_leaf_params'\n",
    "#     print param\n",
    "#     print 'mse mean and std: ',  -scores.mean(), scores.std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0.047711707819577627, 0.01456937594212109) (20, 'auto', 2, 2)\n"
     ]
    }
   ],
   "source": [
    "min_mse = 100\n",
    "min_param = ()\n",
    "for param in result_dict:\n",
    "    if result_dict[param][0] < min_mse:\n",
    "        min_mse=result_dict[param][0]\n",
    "        min_param = param\n",
    "print result_dict[min_param], min_param"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We found that the optimal model uses 20 estimators, 'auto' feature parameters, and will split a minimum of 2 times and leave a minimum of 2 leafs. It is not useful to go into precise defintions of what these terms means. The implications are not clear. We want to focus on the feature importance that will come out as a result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "rf_model = RandomForestRegressor(n_estimators=20, max_features='auto',\n",
    "                                min_samples_split=2, min_samples_leaf=2,random_state=1711, n_jobs=-1)\n",
    "rf_model.fit(x_train, y_train)\n",
    "yhat_rf = rf_model.predict(x_test)\n",
    "#print 'Finished!'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "feature_importance = rf_model.feature_importances_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 log_ebitda/interest_exp 0.1721\n",
      "2 ebitda_margin 0.104\n",
      "3 log_ebitda 0.0983\n",
      "4 t_rev_3_yr_cagr 0.0938\n",
      "5 return_on_assets 0.0923\n"
     ]
    }
   ],
   "source": [
    "rf_feat_pairs = []\n",
    "for i in range(len(feature_importance)):\n",
    "    rf_feat_pairs.append((i,feature_importance[i])) \n",
    "sorted_feat_pairs = sorted(rf_feat_pairs, key=lambda x: abs(x[1]), reverse = True) \n",
    "for i in range(5):\n",
    "    index = sorted_feat_pairs[i][0]\n",
    "    print i+1, X.columns[index], round(sorted_feat_pairs[i][1],4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see again log_ebitda/interest_exp is at the top of the list. There are a few shared features as well such as ebitda_margin, and log_ebitda."
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
