{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Relevant Changes\n",
    "Run this script on all 3 data sets  \n",
    "Sample 25 seeds and calculate a rank for every variable  \n",
    "store these ranks and compute the average score at the end of 25 trials  \n",
    "plot these ranks and their standard errors  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Adamy Valuation - Michigan Data Science Team\n",
    "### Michael Xinyu Tim Sid Derek Manny\n",
    "                                                                                                        5/06/2017"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Purpose:** This notebook describes the data processing pipeline we have built and instructions on how to interpret the results.  \n",
    "  \n",
    "**Conclusion:** Ebitda/interest_exp is the best predictor of ev/ebidta. It seems that the higher the ebitda/interest_exp the lower our prediction for ev/ebitda. Other important features are ebitda_margin and ebitda.  \n",
    "  \n",
    "**Notes:** It was hard to precisely quantify the degree to which these variables actually impacted ev/ebitda because of the limitations of linear regression. We have provided several lists of features that our models found to be important. It would be interesting to compare and contrast these lists with the existing literature. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from get_data import *\n",
    "\n",
    "%matplotlib inline\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pylab as plt\n",
    "import seaborn as sns\n",
    "import scipy.stats as stats\n",
    "import random\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from get_data import *\n",
    "\n",
    "# SciKit Learn Modules\n",
    "from sklearn.preprocessing import scale\n",
    "from sklearn.linear_model import LinearRegression, Ridge, RidgeCV, Lasso, LassoCV\n",
    "from sklearn.model_selection import cross_val_score, train_test_split\n",
    "from sklearn import metrics\n",
    "from sklearn.ensemble import RandomForestRegressor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "np.random.seed(0)\n",
    "raw_data_frame = load_data_frames()\n",
    "sectors = ['consumerDiscrete', 'consumerStaples', 'industrials']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our exploratory data anlaysis led us to discover that our models would be more effective after transforming some of the columns. Specifically those columns that represent monetary values as well as a few others."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0, 't_ev')\n",
      "(1, 't_rev')\n",
      "(2, 'ebitda')\n",
      "(3, 'ebitda_margin')\n",
      "(4, 't_ev/t_rev')\n",
      "(5, 't_ev/ebitda')\n",
      "(6, 'est_ann_rev_gr_1yr')\n",
      "(7, 'est_ann_ebitda_gr_1yr')\n",
      "(8, 't_rev_1yr_growth')\n",
      "(9, 'ebitda_1yr_growth')\n",
      "(10, 't_rev_3_yr_cagr')\n",
      "(11, 'ebitda_3yr_cagr')\n",
      "(12, 't_rev_5yr_cagr')\n",
      "(13, 'ebitda_5yr_cagr')\n",
      "(14, 'return_on_assets')\n",
      "(15, 'return_on_equity')\n",
      "(16, 'capex_as_percent_rev')\n",
      "(17, 'ebitda/interest_exp')\n",
      "(18, 't_debt/cap_percent')\n",
      "(19, 't_debt/equity_percent')\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(raw_data_frame[0].columns)):\n",
    "    print(i, raw_data_frame[0].columns[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['t_ev', 't_rev', 'ebitda', 't_ev/ebitda', 'capex_as_percent_rev', 'ebitda/interest_exp', 't_debt/cap_percent', 't_debt/equity_percent']\n"
     ]
    }
   ],
   "source": [
    "log_transform_columns =  [0, 1, 2, 5, 16, 17, 18, 19]\n",
    "log_transform_col_names = []\n",
    "for col in log_transform_columns:\n",
    "    log_transform_col_names.append(raw_data_frame[0].columns[col])\n",
    "print log_transform_col_names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to actually modify the data for the columns we discovered.  \n",
    "**Note:** Because we do the following process in place it is important that the line is not run twice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# log-transform inplace\n",
    "for i in range(len(sectors)):\n",
    "    raw_data_frame[i][log_transform_col_names] = raw_data_frame[i][log_transform_col_names].apply(np.log10)\n",
    "    raw_data_frame[i].rename(columns = {\n",
    "        't_ev':'log_t_ev',\n",
    "        't_rev':'log_t_rev',\n",
    "        'ebitda':'log_ebitda',\n",
    "        't_ev/ebitda':'log_t_ev/ebitda',\n",
    "        'capex_as_percent_rev':'log_capex_as_percent_rev',\n",
    "        'ebitda/interest_exp':'log_ebitda/interest_exp',\n",
    "        't_debt/cap_percent':'log_t_debt/cap_percent',\n",
    "        't_debt/equity_percent':'log_t_debt/equity_percent'\n",
    "                                },inplace = True)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to handle missing data we fill in cells the with mean for the column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for i in range(len(sectors)):\n",
    "    raw_data_frame[i].fillna(raw_data_frame[i].mean(), inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Prepare the data for ML models\n",
    "\n",
    "- Target: 't_ev/ebitda'\n",
    "- Discard 't_ev'\n",
    "\n",
    "From pearson correlation heatmap, we see that target variable is not strongly linearly associated with other features. Therefore, the nonlinear methods, like tree-based methods, or SVM with nonlinear kernels will outperform. However, one thing to notice is that our final goal is not to predict the target value but rather to understand the complex relationship between ev/ebitda and other features. We shall try various methods  \n",
    "  \n",
    "We chose to discared the feature t_ev and t_ev/t_rev because we believe it will not be known for a private company"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# drop 't_ev' columns\n",
    "for i in range(len(sectors)):\n",
    "    raw_data_frame[i].drop('log_t_ev', axis=1, inplace=True)\n",
    "    raw_data_frame[i].drop('t_ev/t_rev', axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Let's look at consumerDiscrete\n"
     ]
    }
   ],
   "source": [
    "print(\"Let's look at \" + sectors[0])\n",
    "cur_df = raw_data_frame[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>log_t_rev</th>\n",
       "      <th>log_ebitda</th>\n",
       "      <th>ebitda_margin</th>\n",
       "      <th>est_ann_rev_gr_1yr</th>\n",
       "      <th>est_ann_ebitda_gr_1yr</th>\n",
       "      <th>t_rev_1yr_growth</th>\n",
       "      <th>ebitda_1yr_growth</th>\n",
       "      <th>t_rev_3_yr_cagr</th>\n",
       "      <th>ebitda_3yr_cagr</th>\n",
       "      <th>t_rev_5yr_cagr</th>\n",
       "      <th>ebitda_5yr_cagr</th>\n",
       "      <th>return_on_assets</th>\n",
       "      <th>return_on_equity</th>\n",
       "      <th>log_capex_as_percent_rev</th>\n",
       "      <th>log_ebitda/interest_exp</th>\n",
       "      <th>log_t_debt/cap_percent</th>\n",
       "      <th>log_t_debt/equity_percent</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3.075182</td>\n",
       "      <td>1.897077</td>\n",
       "      <td>6.64</td>\n",
       "      <td>4.540000</td>\n",
       "      <td>16.700000</td>\n",
       "      <td>2.06000</td>\n",
       "      <td>-0.852000</td>\n",
       "      <td>16.400000</td>\n",
       "      <td>19.400000</td>\n",
       "      <td>11.200000</td>\n",
       "      <td>18.700000</td>\n",
       "      <td>4.600000</td>\n",
       "      <td>9.900000</td>\n",
       "      <td>0.460898</td>\n",
       "      <td>1.103804</td>\n",
       "      <td>1.513218</td>\n",
       "      <td>1.685742</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2.426511</td>\n",
       "      <td>0.772322</td>\n",
       "      <td>2.22</td>\n",
       "      <td>7.132813</td>\n",
       "      <td>17.742803</td>\n",
       "      <td>-1.93000</td>\n",
       "      <td>13.234825</td>\n",
       "      <td>-4.040000</td>\n",
       "      <td>-34.500000</td>\n",
       "      <td>-10.600000</td>\n",
       "      <td>-28.900000</td>\n",
       "      <td>-1.570000</td>\n",
       "      <td>-12.200000</td>\n",
       "      <td>0.444045</td>\n",
       "      <td>1.004171</td>\n",
       "      <td>1.561461</td>\n",
       "      <td>1.781483</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3.509740</td>\n",
       "      <td>2.665393</td>\n",
       "      <td>14.30</td>\n",
       "      <td>1.480000</td>\n",
       "      <td>4.270000</td>\n",
       "      <td>4.08000</td>\n",
       "      <td>4.980000</td>\n",
       "      <td>13.000000</td>\n",
       "      <td>9.600000</td>\n",
       "      <td>10.300000</td>\n",
       "      <td>11.200000</td>\n",
       "      <td>9.930000</td>\n",
       "      <td>9.950000</td>\n",
       "      <td>0.278754</td>\n",
       "      <td>1.281033</td>\n",
       "      <td>1.489958</td>\n",
       "      <td>1.650308</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3.531900</td>\n",
       "      <td>2.415808</td>\n",
       "      <td>7.65</td>\n",
       "      <td>-5.230000</td>\n",
       "      <td>-36.900000</td>\n",
       "      <td>-3.46000</td>\n",
       "      <td>-19.200000</td>\n",
       "      <td>-7.400000</td>\n",
       "      <td>-21.700000</td>\n",
       "      <td>-3.080000</td>\n",
       "      <td>-15.300000</td>\n",
       "      <td>1.560000</td>\n",
       "      <td>1.330000</td>\n",
       "      <td>0.609594</td>\n",
       "      <td>1.060698</td>\n",
       "      <td>1.311754</td>\n",
       "      <td>1.411620</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3.191898</td>\n",
       "      <td>2.244277</td>\n",
       "      <td>11.30</td>\n",
       "      <td>7.132813</td>\n",
       "      <td>17.742803</td>\n",
       "      <td>331.70347</td>\n",
       "      <td>13.234825</td>\n",
       "      <td>12.670764</td>\n",
       "      <td>8.528391</td>\n",
       "      <td>8.063843</td>\n",
       "      <td>7.665472</td>\n",
       "      <td>6.344578</td>\n",
       "      <td>14.769923</td>\n",
       "      <td>0.187521</td>\n",
       "      <td>0.481443</td>\n",
       "      <td>1.857332</td>\n",
       "      <td>2.411114</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   log_t_rev  log_ebitda  ebitda_margin  est_ann_rev_gr_1yr  \\\n",
       "0   3.075182    1.897077           6.64            4.540000   \n",
       "1   2.426511    0.772322           2.22            7.132813   \n",
       "2   3.509740    2.665393          14.30            1.480000   \n",
       "3   3.531900    2.415808           7.65           -5.230000   \n",
       "4   3.191898    2.244277          11.30            7.132813   \n",
       "\n",
       "   est_ann_ebitda_gr_1yr  t_rev_1yr_growth  ebitda_1yr_growth  \\\n",
       "0              16.700000           2.06000          -0.852000   \n",
       "1              17.742803          -1.93000          13.234825   \n",
       "2               4.270000           4.08000           4.980000   \n",
       "3             -36.900000          -3.46000         -19.200000   \n",
       "4              17.742803         331.70347          13.234825   \n",
       "\n",
       "   t_rev_3_yr_cagr  ebitda_3yr_cagr  t_rev_5yr_cagr  ebitda_5yr_cagr  \\\n",
       "0        16.400000        19.400000       11.200000        18.700000   \n",
       "1        -4.040000       -34.500000      -10.600000       -28.900000   \n",
       "2        13.000000         9.600000       10.300000        11.200000   \n",
       "3        -7.400000       -21.700000       -3.080000       -15.300000   \n",
       "4        12.670764         8.528391        8.063843         7.665472   \n",
       "\n",
       "   return_on_assets  return_on_equity  log_capex_as_percent_rev  \\\n",
       "0          4.600000          9.900000                  0.460898   \n",
       "1         -1.570000        -12.200000                  0.444045   \n",
       "2          9.930000          9.950000                  0.278754   \n",
       "3          1.560000          1.330000                  0.609594   \n",
       "4          6.344578         14.769923                  0.187521   \n",
       "\n",
       "   log_ebitda/interest_exp  log_t_debt/cap_percent  log_t_debt/equity_percent  \n",
       "0                 1.103804                1.513218                   1.685742  \n",
       "1                 1.004171                1.561461                   1.781483  \n",
       "2                 1.281033                1.489958                   1.650308  \n",
       "3                 1.060698                1.311754                   1.411620  \n",
       "4                 0.481443                1.857332                   2.411114  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = cur_df['log_t_ev/ebitda']\n",
    "X = cur_df.drop('log_t_ev/ebitda', axis=1)\n",
    "X.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is common practice in Machine Learning to use inputs of mean = 0 and standard deviation = 1. This way the impact of the features can be compared head to head. The scale() function below does this transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import scale\n",
    "X_scale = scale(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we start doing any model selection it is important that we break our data up into a training and a test set. The training set will be used to select and train the models. The test set will come in at the end of the program and will be our way of testing a models accuracy on data that is has never seen.  \n",
    "The training set will consist of 75% of our data and the test set will have the remaining 25%. Each set has an \"X\" and \"y\" component. The y component is what we are trying to predict, t_ev/ebitda. The X component consists of all other variables in the data set (ebitda, ebitda_margin, est_ann_rev_gr_1yr, etc.) which we will be using to predict t_ev/ebitda."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "x_train, x_test, y_train, y_test = train_test_split(X_scale,y,train_size = 0.75)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forest Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average MSE: 0.0549 (+/- 0.0096)\n"
     ]
    }
   ],
   "source": [
    "# basic model \n",
    "model = RandomForestRegressor()\n",
    "scores = cross_val_score(model, x_train, y_train, scoring='neg_mean_squared_error', cv=5)\n",
    "# print('MSEs:', scores*-1)\n",
    "print(\"Average MSE: %0.4f (+/- %0.4f)\" % (-scores.mean(), scores.std()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are a bunch of 'tuning parameters' for random forest. One commonly used method is to just try a bunch of combinations and take the the one that works best."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from itertools import product\n",
    "n_estimators_params = [10, 20]\n",
    "max_features_params = ['auto', 'sqrt', \"log2\"]\n",
    "min_samples_split_params = [2]\n",
    "min_samples_leaf_params = [  2]\n",
    "\n",
    "result_dict = {}\n",
    "for param in product(n_estimators_params, max_features_params, min_samples_split_params, min_samples_leaf_params):\n",
    "    rf_model = RandomForestRegressor(n_estimators=param[0], max_features=param[1],\n",
    "                                min_samples_split=param[2], min_samples_leaf=param[3], \n",
    "                                 random_state=1711, n_jobs=-1)\n",
    "    scores = cross_val_score(rf_model, x_train, y_train, scoring='neg_mean_squared_error', cv=5)\n",
    "    result_dict[param] = (-scores.mean(), scores.std())\n",
    "#     print 'current param: n_estimators_params, max_features_params, min_samples_split_params, min_samples_leaf_params'\n",
    "#     print param\n",
    "#     print 'mse mean and std: ',  -scores.mean(), scores.std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0.052131293839246737, 0.0087931074800553753) (20, 'sqrt', 2, 2)\n"
     ]
    }
   ],
   "source": [
    "min_mse = 100\n",
    "min_param = ()\n",
    "for param in result_dict:\n",
    "    if result_dict[param][0] < min_mse:\n",
    "        min_mse=result_dict[param][0]\n",
    "        min_param = param\n",
    "print result_dict[min_param], min_param"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We found that the optimal model uses 20 estimators, 'auto' feature parameters, and will split a minimum of 2 times and leave a minimum of 2 leafs. It is not useful to go into precise defintions of what these terms means. The implications are not clear. We want to focus on the feature importance that will come out as a result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "cumulative_winnings = []\n",
    "np.random.seed(212)\n",
    "for j in range(100):\n",
    "    seed = np.random.randint(0,10**4)\n",
    "    x_train, x_test, y_train, y_test = train_test_split(X_scale,y,train_size = 0.75)\n",
    "    rf_model = RandomForestRegressor(n_estimators=20, max_features='auto',\n",
    "                                    min_samples_split=2, min_samples_leaf=2,random_state=seed, n_jobs=-1)\n",
    "\n",
    "    rf_model.fit(x_train, y_train)\n",
    "    yhat_rf = rf_model.predict(x_test)\n",
    "    #print 'Finished!'\n",
    "    feature_importance = rf_model.feature_importances_\n",
    " \n",
    "    rf_feat_pairs = []\n",
    "\n",
    "    for i in range(len(feature_importance)):\n",
    "        rf_feat_pairs.append((X.columns[i],feature_importance[i]))     \n",
    "        cumulative_winnings.append((X.columns[i],feature_importance[i]))\n",
    "\n",
    "\n",
    "# Now I need to average over all instances of column to compute avg score    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df = pd.DataFrame()\n",
    "\n",
    "\n",
    "for i in range(len(X.columns)):\n",
    "    rankings = []\n",
    "    for j in range(len(cumulative_winnings)):\n",
    "        if X.columns[i] == cumulative_winnings[j][0]:\n",
    "            rankings.append(cumulative_winnings[j][1])\n",
    "    df = df.append({'feature': cumulative_winnings[i][0], 'mean_importance': np.mean(rankings), 'se': stats.sem(rankings)}, ignore_index=True)        \n",
    "#     print(cumulative_winnings[i][0])\n",
    "#     print(np.mean(rankings))\n",
    "#     print(np.std(rankings))\n",
    "\n",
    "df = df.sort(\"mean_importance\",ascending=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plot_df = df[0:17]\n",
    "se = plot_df[\"se\"] * 2\n",
    "tuple(se)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "========\n",
    "Barchart\n",
    "========\n",
    "\n",
    "A bar plot with errorbars and height labels on individual bars\n",
    "\"\"\"\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "mean_importance = plot_df[\"mean_importance\"]\n",
    "se = plot_df[\"se\"] * 2\n",
    "\n",
    "ind = np.arange(len(plot_df))  # the x locations for the groups\n",
    "width = 0.35       # the width of the bars\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "rects1 = ax.bar(ind, mean_importance, width, color='r', yerr=tuple(se))\n",
    "\n",
    "\n",
    "\n",
    "# add some text for labels, title and axes ticks\n",
    "ax.set_ylabel('Average Importance')\n",
    "ax.set_title('Feature Importance (100 Trials): Consumer Discrete Sector')\n",
    "plt.xticks(rotation=70)\n",
    "ax.set_xticks(ind + width/2)\n",
    "ax.set_xticklabels(plot_df[\"feature\"])\n",
    "\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
